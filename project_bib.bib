@article{Mearns1996,
	author = {Mearns, Linda O. and Rosenzweig, Cynthia and Goldberg, Richard},
	doi = {10.1007/BF00142465},
	file = {:C$\backslash$:/Users/andre/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mearns, Rosenzweig, Goldberg - 1996 - The effect of changes in daily and interannual climatic variability on CERES-Wheat A sensitivity s.pdf:pdf},
	issn = {0165-0009},
	journal = {Climatic Change},
	mendeley-groups = {Futures/NN},
	month = {mar},
	number = {3},
	pages = {257--292},
	publisher = {Kluwer Academic Publishers},
	title = {{The effect of changes in daily and interannual climatic variability on CERES-Wheat: A sensitivity study}},
	url = {http://link.springer.com/10.1007/BF00142465},
	volume = {32},
	year = {1996}
}
@incollection{Gooding2017,
	abstract = {Achieving quality requires the selection of varieties suited for prevailing environments and cropping systems. For well-adapted varieties, yield and quality can still be affected strongly by the weather and by agronomic interventions. Some of the strongest influences are heat and drought during grain filling, the availability of nitrogen and sulfur, the control of leaf and ear diseases and the control of lodging. The effects of these and other factors are described, particularly in relation to the 'point of sale measures' for wheat grain.},
	author = {Gooding, Michael},
	booktitle = {Cereal Grains},
	doi = {10.1016/B978-0-08-100719-8.00018-8},
	isbn = {9780081007198},
	mendeley-groups = {Futures/NN},
	pages = {493--512},
	publisher = {Elsevier},
	title = {{The Effects of Growth Environment and Agronomy on Grain Quality}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780081007198000188},
	year = {2017}
}
@article{Nuttall2017,
	abstract = {Maintaining grain quality of wheat under climate change is critical for human nutrition, end-use functional properties, as well as commodity value. This paper reviews the current knowledge of high temperature and elevated atmospheric CO2 on whole-grain and functional properties of wheat. It also considers the utility of contemporary crop models for investigating the impacts of climate change on wheat quality; and discusses opportunities for advancing model capability. Under elevated CO2 wheat yield can increase by up to 36{\%}, but universally grain protein concentration decreases and a shift in composition translates to reduced functional properties. High temperature during the post-anthesis period of crops can cause a step change reduction in grain-set, grain size and milling yield. Numerous crop models including APSIM-Nwheat, CropSyst, Sirius, GLAM-HTS account for high CO2 effects through modification of RUE, TE or critical leaf-N concentration and high temperature by accelerated leaf senescence, grain number, potential grain weight or HI modifications. For grain quality, however, crop models are typically restricted to predicting average grain size and grain-N content (concentration), although the SiriusQuality model accounts for the major storage proteins, gliadin and glutenin. For protein composition, high temperature stress reduces the glutenin/gliadin ratio and limits the synthesis of the larger SDS-insoluble glutenin polymers which causes wheat dough to have weaker viscoelasticity properties. This link provides an opportunity to model high temperature effects on grain functional properties. Further development and testing, utilizing grain quality data from global FACE programmes will be particularly valuable for validating and enhancing the performance of such models. For whole-grain characteristics, a single-spike model approach, which accounts for intra-spike variation in assimilate deposition may provide an opportunity to predict grain size distribution and associated screenings percentage and milling yield. Taken together expanding the predictive capability of our crop models to grain quality is an important step in providing a powerful tool for developing adaptation strategies for combating the impacts of climate change to global crop production and grain quality.},
	author = {Nuttall, J.G. and O'Leary, G.J. and Panozzo, J.F. and Walker, C.K. and Barlow, K.M. and Fitzgerald, G.J.},
	doi = {10.1016/j.fcr.2015.12.011},
	file = {:C$\backslash$:/Users/andre/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nuttall et al. - 2017 - Models of grain quality in wheat—A review.pdf:pdf},
	issn = {03784290},
	journal = {Field Crops Research},
	mendeley-groups = {Futures/NN},
	month = {feb},
	pages = {136--145},
	title = {{Models of grain quality in wheat—A review}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0378429015301088},
	volume = {202},
	year = {2017}
}

@misc{CME,
	title = {{Module: Wheat Production, Use, and Transportation - CME Institute}},
	url = {https://institute.cmegroup.com/courses/introduction-to-grains-and-oilseeds/modules/wheat-production-use-and-transportation},
	urldate = {2018-04-28}
}

@misc{InvestopediaRF,
	author = {Investopedia},
	title = {{Roll Forward}},
	url = {https://www.investopedia.com/terms/r/rollforward.asp{\#}ixzz5Do1W1VLq},
	urldate = {2018-04-28}
}

@misc{InvestopediaF,
	author = {Investopedia Futures},
	mendeley-groups = {Futures},
	title = {{Futures Fundamentals}},
	url = {https://www.investopedia.com/university/futures/},
	urldate = {2018-04-28}
}


@inproceedings{YoungohcYoon,
	author = {{Youngohc Yoon} and Swales, G.},
	booktitle = {Proceedings of the Twenty-Fourth Annual Hawaii International Conference on System Sciences},
	doi = {10.1109/HICSS.1991.184055},
	pages = {156--162},
	publisher = {IEEE Comput. Soc. Press},
	title = {{Predicting stock price performance: a neural network approach}},
	url = {http://ieeexplore.ieee.org/document/184055/},
	volume = {iv}
}
@article{Ugurlu2018,
	author = {Ugurlu, Umut and Oksuz, Ilkay and Tas, Oktay},
	doi = {10.20944/PREPRINTS201804.0286.V1},
	file = {:C$\backslash$:/Users/andre/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ugurlu, Oksuz, Tas - 2018 - Electricity Price Forecasting Using Recurrent Neural Networks.pdf:pdf},
	keywords = {artificial intelligence, turkish day-ahead market,deep learning,electricity price forecasting,gated recurrent units,long short term memory},
	month = {apr},
	publisher = {Preprints},
	title = {{Electricity Price Forecasting Using Recurrent Neural Networks}},
	url = {https://www.preprints.org/manuscript/201804.0286/v1},
	year = {2018}
}
@article{Chung,
	abstract = {In this work, we propose a novel recurrent neu-ral network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recur-rent layers by allowing and controlling signals flowing from upper recurrent layers to lower lay-ers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evalu-ated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional ap-proaches to build deep stacked RNNs. We sug-gest that the improvement arises because the GF-RNN can adaptively assign different layers to dif-ferent timescales and layer-to-layer interactions (including the top-down ones which are not usu-ally present in a stacked RNN) by learning to gate these interactions.},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
	file = {:C$\backslash$:/Users/andre/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chung et al. - Unknown - Gated Feedback Recurrent Neural Networks.pdf:pdf},
	title = {{Gated Feedback Recurrent Neural Networks}},
	url = {http://proceedings.mlr.press/v37/chung15.pdf}
}
@article{Schmidhuber2015,
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	archivePrefix = {arXiv},
	arxivId = {1404.7828},
	author = {Schmidhuber, J{\"{u}}rgen},
	doi = {10.1016/j.neunet.2014.09.003},
	eprint = {1404.7828},
	issn = {1879-2782},
	journal = {Neural networks : the official journal of the International Neural Network Society},
	month = {jan},
	pages = {85--117},
	pmid = {25462637},
	title = {{Deep learning in neural networks: an overview.}},
	url = {http://arxiv.org/abs/1404.7828 http://dx.doi.org/10.1016/j.neunet.2014.09.003 http://www.ncbi.nlm.nih.gov/pubmed/25462637},
	volume = {61},
	year = {2015}
}
@article{Hochreiter1997,
	abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called $\backslash$Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error through $\backslash$constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
	author = {Hochreiter, Sepp and {Urgen Schmidhuber}, Jj},
	file = {::},
	journal = {Neural Computation},
	number = {8},
	pages = {1735--1780},
	title = {{LONG SHORT-TERM MEMORY}},
	url = {http://www7.informatik.tu-muenchen.de/{~}hochreit http://www.idsia.ch/{~}juergen},
	volume = {9},
	year = {1997}
}

@misc{chollet2015keras,
	title={Keras},
	author={Chollet, Fran\c{c}ois and others},
	year={2015},
	howpublished={\url{https://keras.io}},
}

  @Article{ameila,
	title = {{Amelia II}: A Program for Missing Data},
	author = {James Honaker and Gary King and Matthew Blackwell},
	journal = {Journal of Statistical Software},
	year = {2011},
	volume = {45},
	number = {7},
	pages = {1--47},
	url = {http://www.jstatsoft.org/v45/i07/},
}
